"""
text_report.py ‚Äî –æ—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –õ–†4.
–°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–∞–º –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á—ë—Ç –≤ CSV.
"""

from lab04.io_txt_csv import read_text, write_csv
from lib.text import normalize, tokenize, count_freq, top_n
from pathlib import Path

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
DATA_DIR = Path("src/lab04/data")
INPUT_FILE = DATA_DIR / "input.txt"
OUTPUT_FILE = DATA_DIR / "report.csv"


def prepare_input():
    """
    –°–æ–∑–¥–∞—ë—Ç –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –æ–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.
    –≠—Ç–æ —É–¥–æ–±–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã.
    """
    sample_text = """Python ‚Äî —ç—Ç–æ –º–æ—â–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.
    –û–Ω –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞, –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏.
    Python –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –Ω–∞—É–∫–µ, –±–∏–∑–Ω–µ—Å–µ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏."""
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    if not INPUT_FILE.exists():
        INPUT_FILE.write_text(sample_text, encoding="utf-8")
        print("üìÑ –§–∞–π–ª input.txt —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
    else:
        print("üìÑ –§–∞–π–ª input.txt —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π.")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã:
    1. –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.
    2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è.
    3. –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏.
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV –∏ –≤—ã–≤–æ–¥ –Ω–∞ —ç–∫—Ä–∞–Ω.
    """
    prepare_input()  # —Å–æ–∑–¥–∞—ë–º input.txt –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

    # –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    text = read_text(INPUT_FILE)
    print("\n=== –®–∞–≥ 1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ===")
    print(text)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, –∑–∞–º–µ–Ω–∞ '—ë' –Ω–∞ '–µ')
    normalized = normalize(text)
    print("\n=== –®–∞–≥ 2. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ===")
    print(normalized)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞
    tokens = tokenize(normalized)
    print(f"\n=== –®–∞–≥ 3. –í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)} ===")
    print(tokens[:10], "...")  # –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤

    # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
    freqs = count_freq(tokens)
    top_words = top_n(freqs, 10)

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n=== –®–∞–≥ 4. –¢–æ–ø-10 —Å–ª–æ–≤ ===")
    for word, count in top_words:
        print(f"{word:15} | {count}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV
    write_csv(OUTPUT_FILE, top_words)
    print("\n‚úÖ –ü—Ä–æ–≥—Ä–∞–º–º–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


# –ó–∞–ø—É—Å–∫
if __name__ == "__main__":
    main()